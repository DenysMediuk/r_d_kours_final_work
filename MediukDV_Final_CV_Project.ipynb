{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3e0ac4",
   "metadata": {},
   "source": [
    "# Detection  of demaged parts on conveer\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2333966",
   "metadata": {},
   "source": [
    "### Motivation :\n",
    "\n",
    "In this work we will make the computer vision program what can recognize normal parts 'ok' and demaged parts 'nok', sand signal for rlrctromechanics part on conveyor and srparete two tipes of parts and count the damaged parts for statistics. \n",
    "\n",
    "As a result we must get redusing a labor cost in conveyor production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54dcc5",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "To get result we nead to make next steps:\n",
    " - 1. Make our oun dataset with labling a both classes ('ok', 'nok');\n",
    " - 2. Chose neural network, way of training and presets to get sutable result with small dataset.\n",
    " - 3. Make the working program to detect on a 'nok' parts and signalise to electro-mechanical part to separate 'ok'       and 'nok' pants with counter to make statistics for Quality Engineer (QE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f6eae",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "1. Dataset:\n",
    "Images for dataset i make from digital camera with IR backlight in constant conditions and distance, thet why i get the same quality of pictures for training and in real application, but there is a limit number of a parts, thet why the size of a dataset is not large. (1000+ pictures. 370 - 'ok', 370- 'nok', and 270 - 4 mixed parts in a row in ewery picture)\n",
    "2. Type of neural network: \n",
    "I tried a few tipes of neural networks for example, but SNN can't handle it with small dataset and diferent positions of parts with diferent angles. CNN like YOLOv8n did it well in a real time , so i chose it. \n",
    "3. Training of neural network: \n",
    "I chosed training in a 4 stage with 250 epochs with hiper functions with augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0688a",
   "metadata": {},
   "source": [
    "# Training program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a648b6f4",
   "metadata": {},
   "source": [
    "#### Part 0: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import ast\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = [20, 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381e472",
   "metadata": {},
   "source": [
    "#### Part 1: Verify CUDA Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b31632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CUDA availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622b01f",
   "metadata": {},
   "source": [
    "#### Part 2: Define Paths and Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"/home/CV/Computer-Vision-v2-master/lesson_18/data/detect.v2i.yolov8_1000/data.yaml\"\n",
    "MODEL_WEIGHTS = \"yolov8n.pt\"  # Pretrained weights\n",
    "PROJECT_DIR = \"/home/CV/Computer-Vision-v2-master/lesson_18/data/results\"\n",
    "EXPERIMENT_NAME = \"yolov8_experiment_progressive\"\n",
    "EPOCHS_PER_STAGE = 250  # Number of epochs per stage\n",
    "\n",
    "\n",
    "# Finding the latest experiment directory\n",
    "\n",
    "\n",
    "def find_latest_experiment(project_dir, experiment_name):\n",
    "    project_path = Path(project_dir)\n",
    "    experiment_dirs = sorted(project_path.glob(f\"{experiment_name}*\"), key=os.path.getmtime, reverse=True)\n",
    "    if not experiment_dirs:\n",
    "        raise FileNotFoundError(f\"No experiment directories found with name starting with '{experiment_name}' in {project_dir}\")\n",
    "    return experiment_dirs[0]\n",
    "\n",
    "\n",
    "# Runing a training stage with augmentations\n",
    "\n",
    "\n",
    "def run_training_stage(stage_num, initial_weights, epochs=250):\n",
    "    current_experiment_name = f\"{EXPERIMENT_NAME}_stage{stage_num}\"\n",
    "    \n",
    "    # Initializing the model with the initial_weights\n",
    "    model = YOLO(initial_weights)\n",
    "    \n",
    "    # Train the model. We add additional augmentation parameters:\n",
    "    # - degrees: rotate images by up to ±10 degrees\n",
    "    # - fliplr: probability of horizontal flip\n",
    "    # - flipud: probability of vertical flip\n",
    "    # - scale: scale images by ±50%\n",
    "    model.train(\n",
    "        data=DATA_PATH,\n",
    "        epochs=epochs,\n",
    "        imgsz=640,\n",
    "        batch=8,\n",
    "        name=current_experiment_name,\n",
    "        workers=8,\n",
    "        project=PROJECT_DIR,\n",
    "        device=device,\n",
    "        verbose=True,\n",
    "        save=True,\n",
    "        save_period=-1,\n",
    "        optimizer='auto',\n",
    "        seed=0,\n",
    "        deterministic=True,\n",
    "        resume=False,\n",
    "        cache=False,\n",
    "        augment=True,\n",
    "        multi_scale=True,\n",
    "        amp=True,\n",
    "        degrees=10,\n",
    "        fliplr=0.5,\n",
    "        flipud=0.5,\n",
    "        scale=0.5\n",
    "    )\n",
    "\n",
    "    # Finding the best model from this stage\n",
    "    latest_experiment_dir = find_latest_experiment(PROJECT_DIR, current_experiment_name)\n",
    "    best_model_path = latest_experiment_dir / \"weights\" / \"best.pt\"\n",
    "\n",
    "    if not best_model_path.exists():\n",
    "        raise FileNotFoundError(f\"The best.pt file was not found at {best_model_path} after stage {stage_num}.\")\n",
    "\n",
    "    print(f\"Stage {stage_num} best model path: {best_model_path}\")\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb430834",
   "metadata": {},
   "source": [
    "#### Part 3: Run Multiple Stages of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3843e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Start from the pretrained model weights\n",
    "best_model_path_stage1 = run_training_stage(stage_num=1, initial_weights=MODEL_WEIGHTS, epochs=EPOCHS_PER_STAGE)\n",
    "\n",
    "# Stage 2: Start from best model of stage 1\n",
    "best_model_path_stage2 = run_training_stage(stage_num=2, initial_weights=best_model_path_stage1, epochs=EPOCHS_PER_STAGE)\n",
    "\n",
    "# Stage 3: Start from best model of stage 2\n",
    "best_model_path_stage3 = run_training_stage(stage_num=3, initial_weights=best_model_path_stage2, epochs=EPOCHS_PER_STAGE)\n",
    "\n",
    "# Stage 4: Start from best model of stage 3\n",
    "best_model_path_stage4 = run_training_stage(stage_num=4, initial_weights=best_model_path_stage3, epochs=EPOCHS_PER_STAGE)\n",
    "\n",
    "# Stage 5: Start from best model of stage 4\n",
    "best_model_path_stage5 = run_training_stage(stage_num=5, initial_weights=best_model_path_stage4, epochs=EPOCHS_PER_STAGE)\n",
    "\n",
    "# After finishing all stages, the best model from the last stage is your final model.\n",
    "final_best_model_path = best_model_path_stage5\n",
    "print(f\"Final best model after 5 stages: {final_best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934166c7",
   "metadata": {},
   "source": [
    "#### Part 4: Run Inference on the Test Set with the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = YOLO(final_best_model_path)\n",
    "TEST_IMAGES_DIR = \"/home/CV/Computer-Vision-v2-master/lesson_18/data/detect.v2i.yolov8_1000/test/images\"\n",
    "TEST_PREDICTIONS_DIR = Path(PROJECT_DIR) / f\"{EXPERIMENT_NAME}_test_predictions\"\n",
    "\n",
    "predictions = best_model.predict(\n",
    "    source=TEST_IMAGES_DIR,\n",
    "    save=True,\n",
    "    save_txt=True,\n",
    "    save_conf=True,\n",
    "    save_crop=True,\n",
    "    imgsz=640,\n",
    "    conf=0.7,\n",
    "    iou=0.75,\n",
    "    device=device,\n",
    "    exist_ok=True,\n",
    "    project=PROJECT_DIR,\n",
    "    name=f\"{EXPERIMENT_NAME}_test_predictions\"\n",
    ")\n",
    "\n",
    "print(f\"Inference completed. Predictions saved to {TEST_PREDICTIONS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc92a3d",
   "metadata": {},
   "source": [
    "#### Part 5: Evaluate the Model on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_TEST_DATA_YAML = \"/home/CV/Computer-Vision-v2-master/lesson_18/data/detect.v2i.yolov8_1000/test_data.yaml\"\n",
    "\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    data_yaml = yaml.safe_load(f)\n",
    "\n",
    "test_data_yaml = {\n",
    "    'train': data_yaml.get('train', '../train/images'),\n",
    "    'val': 'test/images',\n",
    "    'test': 'test/images',\n",
    "    'nc': data_yaml['nc'],\n",
    "    'names': data_yaml['names']\n",
    "}\n",
    "\n",
    "# Optionally, include roboflow metadata if present\n",
    "if 'roboflow' in data_yaml:\n",
    "    test_data_yaml['roboflow'] = data_yaml['roboflow']\n",
    "\n",
    "with open(TEMP_TEST_DATA_YAML, 'w') as f:\n",
    "    yaml.dump(test_data_yaml, f)\n",
    "\n",
    "print(f\"Temporary test data.yaml created at {TEMP_TEST_DATA_YAML}\")\n",
    "\n",
    "test_results = best_model.val(data=TEMP_TEST_DATA_YAML)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d021c816",
   "metadata": {},
   "source": [
    "#### Part 6: Print Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest Set Evaluation Metrics:\")\n",
    "try:\n",
    "    metrics_dict = test_results.results_dict\n",
    "    for key, value in metrics_dict.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "except AttributeError as e:\n",
    "    print(\"Error accessing metrics:\", e)\n",
    "    print(\"Available keys:\", test_results.keys())\n",
    "    print(\"Maps:\", test_results.maps)\n",
    "    print(\"Fitness:\", test_results.fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f88b99b",
   "metadata": {},
   "source": [
    "#### Part 8: Examples of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictions:\n",
    "    for i, result in enumerate(predictions[:3]):\n",
    "        print(f\"\\nPrediction {i+1} Details:\")\n",
    "        print(f\"Image shape: {result.orig_shape}\")\n",
    "        print(\"Detections:\")\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls)\n",
    "            confidence = float(box.conf)\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "            class_name = data_yaml['names'][cls_id] if cls_id < len(data_yaml['names']) else \"unknown\"\n",
    "            print(f\" - {class_name} at [{x1:.2f}, {y1:.2f}, {x2:.2f}, {y2:.2f}] (conf: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf37f8",
   "metadata": {},
   "source": [
    "val: Scanning /home/st/CV/Computer-Vision-v2-master/lesson_18/data/detect.v2i.yo\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
    "\n",
    "                   all        105        180      0.984      0.994      0.989      0.826\n",
    "                   nok         68         68      0.985      0.998      0.984      0.799\n",
    "                    ok         64        112      0.983      0.991      0.995      0.852\n",
    "Speed: 0.2ms preprocess, 4.0ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
    "Results saved to runs/detect/val15\n",
    "\n",
    "Test Set Evaluation Metrics:\n",
    "metrics/precision(B): 0.9843869703645236\n",
    "metrics/recall(B): 0.9943543397243358\n",
    "metrics/mAP50(B): 0.989431288819876\n",
    "metrics/mAP50-95(B): 0.8258556818617497\n",
    "fitness: 0.8422132425575625\n",
    "\n",
    "\n",
    "Prediction 1 Details:\n",
    "Image shape: (640, 640)\n",
    "Detections:\n",
    " - ok at [317.43, 138.53, 351.72, 183.33] (conf: 0.89)\n",
    "\n",
    "Prediction 2 Details:\n",
    "Image shape: (640, 640)\n",
    "Detections:\n",
    " - ok at [283.14, 244.39, 317.99, 289.41] (conf: 0.90)\n",
    "\n",
    "Prediction 3 Details:\n",
    "Image shape: (640, 640)\n",
    "Detections:\n",
    " - ok at [272.64, 384.43, 308.51, 428.58] (conf: 0.90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af9074",
   "metadata": {},
   "source": [
    "# Detection program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba474e",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbffc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad4ee32",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = \"/home/st/CV/Computer-Vision-v2-master/lesson_18/data/results/yolov8_experiment_progressive_stage402/weights/best.pt\"  # Path to your trained YOLOv8 model\n",
    "VIDEO_INPUT_PATH = \"/home/st/CV/Computer-Vision-v2-master/lesson_18/data/detect.v1i.yolov8/video/20241222_20_16_04.mp4\"  # Input .mp4 video path\n",
    "VIDEO_OUTPUT_PATH = \"/home/st/CV/Computer-Vision-v2-master/lesson_18/data/detect.v1i.yolov8/output_video/output_video.mp4\"  # Output .mp4 video path\n",
    "CONF_THRESHOLD = 0.80  # Confidence threshold for detections\n",
    "IOU_THRESHOLD = 0.85   # IoU threshold for Non-Max Suppression\n",
    "FONT_PATH = None        # Optional: Path to a .ttf font file for labels (e.g., \"/path/to/font.ttf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508d3e6",
   "metadata": {},
   "source": [
    "#### Counter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COUNT = 100000       # Maximum count value\n",
    "counter = 0              # Initialize counter\n",
    "square_on = False        # Flag to indicate if the red square is currently displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e71e2",
   "metadata": {},
   "source": [
    "#### Initializing Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9bad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425852b9",
   "metadata": {},
   "source": [
    "#### Loading the YOLOv8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = YOLO(BEST_MODEL_PATH)\n",
    "    model.to(device)\n",
    "    names = model.names  # Class names from the model\n",
    "    print(\"YOLOv8 model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YOLOv8 model: {e}\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cda27c",
   "metadata": {},
   "source": [
    "#### Define Class Color Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc430203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each class. Modify this dictionary based on your actual class names.\n",
    "# For example, if your classes are ['nok', 'ok'], map them accordingly.\n",
    "class_color_map = {\n",
    "    'nok': 'red',\n",
    "    'ok': 'green'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece314a",
   "metadata": {},
   "source": [
    "#### Drawing a Bounding Boxes and Signalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8efed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes_on_frame(frame, boxes, names, class_color_map, font_path=None):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and labels on the frame. Additionally, draws a vertical line and\n",
    "    a red square in the top-left corner if a 'nok' object crosses the line.\n",
    "    \n",
    "    Parameters:\n",
    "    - frame (numpy.ndarray): The image frame in BGR format.\n",
    "    - boxes (BoxList): The detected bounding boxes.\n",
    "    - names (list): List of class names.\n",
    "    - class_color_map (dict): Mapping from class names to colors.\n",
    "    - font_path (str): Path to the .ttf font file.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: The annotated frame in BGR format.\n",
    "    - bool: Flag indicating if a 'nok' object has crossed the line.\n",
    "    \"\"\"\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    draw_obj = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    width, height = img.size\n",
    "    x_line = width / 2  # Vertical line in the center\n",
    "    \n",
    "    # Draw the vertical line (top to bottom) with 2-pixel thickness\n",
    "    line_color = 'blue'  # Color for the vertical line\n",
    "    draw_obj.line([(x_line, 0), (x_line, height)], fill=line_color, width=2)\n",
    "    \n",
    "    # Flag to check if any 'nok' object crosses the line\n",
    "    nok_crossed = False\n",
    "    \n",
    "    # Load a font if FONT_PATH is provided; else use default\n",
    "    if font_path and Path(font_path).exists():\n",
    "        try:\n",
    "            font = ImageFont.truetype(font_path, 20)\n",
    "        except IOError:\n",
    "            print(\"Custom font not found. Using default font.\")\n",
    "            font = ImageFont.load_default()\n",
    "    else:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Iterate through each detected box\n",
    "    for box in boxes:\n",
    "        cls_id = int(box.cls)\n",
    "        conf = float(box.conf)\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        class_name = names[cls_id] if cls_id < len(names) else \"unknown\"\n",
    "        label = f\"{class_name} {conf:.2f}\"\n",
    "        \n",
    "        # Determine color based on class\n",
    "        color = class_color_map.get(class_name, 'blue')  # Default to blue if class not mapped\n",
    "        \n",
    "        # Draw bounding box\n",
    "        draw_obj.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "        \n",
    "        # Calculate text size using a try-except block for compatibility\n",
    "        try:\n",
    "            text_w, text_h = draw_obj.textsize(label, font=font)\n",
    "        except AttributeError:\n",
    "            # For newer Pillow versions\n",
    "            bbox = draw_obj.textbbox((0, 0), label, font=font)\n",
    "            text_w = bbox[2] - bbox[0]\n",
    "            text_h = bbox[3] - bbox[1]\n",
    "        \n",
    "        # Draw label background\n",
    "        draw_obj.rectangle([x1, y1 - text_h - 4, x1 + text_w + 4, y1], fill=color)\n",
    "        # Draw label text\n",
    "        draw_obj.text((x1 + 2, y1 - text_h - 2), label, fill=\"white\", font=font)\n",
    "        \n",
    "        # Check if 'nok' object crosses the vertical line\n",
    "        if class_name == 'nok':\n",
    "            # Define a small tolerance (e.g., 2 pixels)\n",
    "            tolerance = 2\n",
    "            if (x1 - tolerance) <= x_line <= (x2 + tolerance):\n",
    "                nok_crossed = True\n",
    "\n",
    "    # If any 'nok' object crosses the line, draw a red square in the top-left corner\n",
    "    if nok_crossed:\n",
    "        square_size = 50\n",
    "        square_color = 'red'\n",
    "        draw_obj.rectangle([0, 0, square_size, square_size], fill=square_color)\n",
    "    \n",
    "    # Convert back to BGR for OpenCV\n",
    "    annotated_frame = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return annotated_frame, nok_crossed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1001e",
   "metadata": {},
   "source": [
    "#### Initializinf Video Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a04a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_video_capture(video_path):\n",
    "    \"\"\"\n",
    "    Initializes the video capture object for the given video file.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_path (str): Path to the input video file.\n",
    "    \n",
    "    Returns:\n",
    "    - cap (cv2.VideoCapture): The video capture object.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open video file: {video_path}\")\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67470536",
   "metadata": {},
   "source": [
    "#### Real-Time Video Processing with Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_file():\n",
    "    \"\"\"\n",
    "    Processes the input video file frame by frame, performing object detection,\n",
    "    counting 'nok' object crossings, and saving the annotated video.\n",
    "    \"\"\"\n",
    "    global counter, square_on  # To modify the global counter and square_on variables\n",
    "    cap = initialize_video_capture(VIDEO_INPUT_PATH)\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        fps = 30  # Default to 30 if unable to get FPS\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video Properties:\\n - Resolution: {width}x{height}\\n - FPS: {fps}\\n - Total Frames: {total_frames}\")\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    output_dir = Path(VIDEO_OUTPUT_PATH).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "    print(\"Starting video processing...\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "        frame_count += 1\n",
    "        \n",
    "        # Run inference on the frame\n",
    "        results = model.predict(\n",
    "            frame,\n",
    "            conf=CONF_THRESHOLD,\n",
    "            iou=IOU_THRESHOLD,\n",
    "            device=device,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        dets = results[0]  # detections for this frame\n",
    "        annotated_frame, nok_crossed = draw_boxes_on_frame(frame, dets.boxes, names, class_color_map, FONT_PATH)\n",
    "        \n",
    "        # Update counter and square_on flag\n",
    "        if nok_crossed and not square_on:\n",
    "            counter += 1\n",
    "            square_on = True\n",
    "            print(f\"'nok' object crossed the line. Current count: {counter}\")\n",
    "            if counter >= MAX_COUNT:\n",
    "                print(f\"Maximum count of {MAX_COUNT} reached.\")\n",
    "                counter = MAX_COUNT  # Prevent counter from exceeding MAX_COUNT\n",
    "        elif not nok_crossed and square_on:\n",
    "            square_on = False  # Reset the flag when no crossing is detected\n",
    "        \n",
    "        # Draw the counter at the top center of the frame\n",
    "        img_pil = Image.fromarray(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
    "        draw = ImageDraw.Draw(img_pil)\n",
    "        \n",
    "        # Load a font for the counter\n",
    "        if FONT_PATH and Path(FONT_PATH).exists():\n",
    "            try:\n",
    "                font = ImageFont.truetype(FONT_PATH, 25)\n",
    "            except IOError:\n",
    "                print(\"Custom font not found. Using default font.\")\n",
    "                font = ImageFont.load_default()\n",
    "        else:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        counter_text = f\"Count: {counter}\"\n",
    "        try:\n",
    "            text_w, text_h = draw.textsize(counter_text, font=font)\n",
    "        except AttributeError:\n",
    "            # For newer Pillow versions\n",
    "            bbox = draw.textbbox((0, 0), counter_text, font=font)\n",
    "            text_w = bbox[2] - bbox[0]\n",
    "            text_h = bbox[3] - bbox[1]\n",
    "        \n",
    "        # Calculate position: top center\n",
    "        text_x = (width - text_w) / 2\n",
    "        text_y = 10  # 10 pixels from the top\n",
    "        \n",
    "        # Draw a solid rectangle behind the text for better visibility\n",
    "        draw.rectangle([text_x - 5, text_y - 5, text_x + text_w + 5, text_y + text_h + 5], fill=(0, 0, 0))\n",
    "        # Draw the counter text\n",
    "        draw.text((text_x, text_y), counter_text, fill=\"yellow\", font=font)\n",
    "        \n",
    "        # Convert back to BGR for OpenCV\n",
    "        annotated_frame_with_counter = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(annotated_frame_with_counter)\n",
    "        \n",
    "        # Optional: Display progress every 100 frames\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Processed {frame_count}/{total_frames} frames in {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nProcessing completed. Output saved to: {VIDEO_OUTPUT_PATH}\")\n",
    "    print(f\"Total Frames Processed: {frame_count}\")\n",
    "    print(f\"Total Time Taken: {total_time:.2f} seconds\")\n",
    "    print(f\"Average FPS: {frame_count / total_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0d012",
   "metadata": {},
   "source": [
    "#### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c4031",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        process_video_file()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab6b80",
   "metadata": {},
   "source": [
    "#### Print: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4641aaa",
   "metadata": {},
   "source": [
    "Using device: cuda\n",
    "CUDA available: True\n",
    "YOLOv8 model loaded successfully.\n",
    "Video Properties:\n",
    " - Resolution: 640x480\n",
    " - FPS: 12.53066199116487\n",
    " - Total Frames: 199\n",
    "Starting video processing...\n",
    "'nok' object crossed the line. Current count: 1\n",
    "'nok' object crossed the line. Current count: 2\n",
    "'nok' object crossed the line. Current count: 3\n",
    "'nok' object crossed the line. Current count: 4\n",
    "'nok' object crossed the line. Current count: 5\n",
    "'nok' object crossed the line. Current count: 6\n",
    "Processed 100/199 frames in 2.06 seconds.\n",
    "'nok' object crossed the line. Current count: 7\n",
    "'nok' object crossed the line. Current count: 8\n",
    "'nok' object crossed the line. Current count: 9\n",
    "\n",
    "Processing completed. Output saved to: /home/st/CV/Computer-Vision-v2-master/lesson_18/data/detect.v1i.yolov8/output_video/output_video.mp4\n",
    "Total Frames Processed: 199\n",
    "Total Time Taken: 4.02 seconds\n",
    "Average FPS: 49.47"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cvpy38env)",
   "language": "python",
   "name": "cvpy38env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
